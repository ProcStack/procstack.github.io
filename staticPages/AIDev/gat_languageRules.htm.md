# GAT & Language

For my 2026 goal, I've been exploring Graph Attention Network (GAT) artificial intelligence.
       As GATs allow me to treat 'concepts' as 'objects', rather than sections of words/pixels as a tensor or 'piece of a concept'.

        GATs are a type of neural network that considers the relationships between data points.
        
As a type of Graph Neural Network (GNN),
        
Its best for predicting connections between ideas / things / data in a system.

        GNNs are commonly used for "Recommendation Systems",
        
     Hey, you might know Jim Bob McGee!!
        
   But GATs could be used for so much more!

I've been working on a general-purpose neuron that adjusts its own connections during prediction;
      
   So the same system could learn my voice on the fly, as well as sensor signals connected to the Jetson computer.

Since its the Structure in a GAT that causes regions of neural activation based on stimuli,
      
   It forms a result (prediction) after subsequent activations, as-though compounding ripples in a pond.

Rather than a field of numbers aligning to yield a prediction,
      
   It's the structure of neural connections which manipulates the data.

I've been going in a direction that should yield a similar result to a Recurrent Neural Network (RNN), but with a different mental structure.
      
   With that general-purpose neuron, I can provide text, images, audio histograms, etc. to the network.

RNNs can be used for/in many types of ai,
        
Best for detecting patterns in sequential data,
        
Like time-based events or words in text.

        They are the basis for many types of ai, like LSTMs.

 The GAT will create connections from initial random data points, sample the differences, then pass the 'prediction' forward and 'back' in the chain, and adjust the connections based on their revisit to the same data in the current 'prediction'.
      
   Relying on localized regions of sub-networks to recurrently process the data

It should be self-taught discrimination of attention between neurons;
      
   Like in the human brain.
      
     (When the purple circles go red in the GAT video, first vid)