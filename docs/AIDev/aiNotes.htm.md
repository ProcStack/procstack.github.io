# Notes & Research

I made a whole 'blog' system for this... yet here we are....
      
   ( nudge, nudge )

If you couldn't tell, I'm training my AIs on my own works.
      
A personally made AI trained on personally made images / videos / photos / code / writing.
      
   That means I can copyright my generations, right?
      
     If I made every aspect of the AI & training data?

      - February 2025

I've begun on the core of the AI, as of May 24th, 2025.
      
   I have the beginnings of a 'Micro-Term' memory implemented to act as a gated-attention during inference.
      
This, paired with automatic graph edge splitting ('Dynamic' in DGNN or DGAT) and use of geometric clustering, seems to be giving me values of a "remembered" object when it's outside of the dataset.
      
   Hopefully leading to bodily awareness of limbs, objects outside of the field of view, and other 'long term' tensors/classifications at a temporary scale.

It's a 4d kernel, in that it uses an ESN to train on it's own mistakes,
      
   Basing it's decisions on prior back-propagation states/adjustments.
      
   The beginnings of a meta-learning process, I hope!

I'm using a method I'm calling 'Stack Crunching',
      
   Where I agregate the time dependent weights into a "checkpoint" of sorts.
      
   This allows the ESN to have a 'baseline' understanding of data that I can parse into with vectors calculated from tensor weights found within a quantized version of the input data.

You can assume that the 'ESN' is not a standard 'Echo State Network' anymore.
      - May 2025

 With a bit more research into the types of minds that brought us DeepMind, and their work on GNN networks,
      
 I read a bit of Petar Velickovic's work on topological deep learning and the geometry of GNNs.
      
 Coming to find out that my idea of 'Stack Crunching' is similar to 'Squashing' in GNNs.

 So I've been inspired to propperly name my neural structure-
      
It's a Dynamic Pointer-Attention Message Passing Neural Network with Affine-Projections
      
   or a dPA-MPNN

 But I must say, this isn't Affine Projections like in the papers,
      
   It's more like a 'projection' of the data into 'pointer' space;
      
   Actual Affine Matricies.
      
     I am a Technical Artist first before an AI Researcher after all, BOIDS!

It all comes down to BOOOOIIIIIDDDDSSSSSS instead of Adam, baby!
      
Because, what is Adam? It's a direction to move in a field of numbers, with momentum and a learning rate.
      
   Yet... That's just a simple Boid, now isn't it?
      
   Just without a few of the more advanced rules, which make boids feel so alive!

Having some Tiny Brains running around in hyperdimensional space like little buggers running around avoiding each other.
      
   Because if they collide, double activation happens when it may not be desired.
      
   (I'm happy I finally saw a paper on Tiny Brains, giving my idea credence, cause it fits! .. in my mind.)
      
(Only difference is that it was a [study](https://www.nature.com/articles/s41586-025-09142-4) into small biological systems, not artificial ones... but I'm gettin there!)
      - July 2025

I'd like to believe I'm moving in the right direction with the feedback systems I'm developing.
      
   But been further creating other architectures to see how they operate.

I created a GAN for upressing, which helped me understand a bit better the pairing of mental structures between both our brain's hemispheres.
      
   So I added a time based memory to check if the training was moving in the right direction.
      
     It definitely helped guide training a bit quicker.

Shows my knowledge base that I'm impressed by back-up supported learning...
      
   But'is proof of concept!

Adversarial networks exist in nature to guide a 'single' thought's path.
      
   Yet in the case of Group Think between humans,
      
     Balance is never reached.
      - August 2025