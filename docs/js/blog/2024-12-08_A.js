import{blogEntry}from"./blogEntryBase.js";let entryTitle="State of AI; Late 2024",entryDate="2024-12-08",entryTags=["ai","machine learning","neural networks","deep learning","artificial intelligence","decemeber","2024"],entryBody='\nThere is something to be said about the path AI has been going on since the mid-late 2010s.\n<br>\n<br>This isn\'t about the data scraping or privacy issues, much of that was a wash because of internet law set back in the 90s.\n<br>We walked into our data being scraped, and legally at that.\n<br>It\'s just morally questionable.\n<br>\n<div class="blogSpacer"></div>\n\nI\'d like to comment on the path we are taking with AI.\n<br>I just don\'t think Transformers and U-Nets are the end-all be-all of it,\n<br>&nbsp;&nbsp; I\'d imagine rightfully so, since we are pretty early on here.\n<br>&nbsp;&nbsp;&nbsp;&nbsp; This is where ai Agents come in, and <a href="https://en.wikipedia.org/wiki/Mixture_of_experts" target="_blank" alt="Mixture of Experts">Mixture of Experts</a> (MoE).\n<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; To fill the gaps.\n<br>\n<br>The network type and model for General Intelligence\n<br>&nbsp;&nbsp; And the new "Super Intelligence" just hasn\'t been found/developed yet.\n<br>\n<br>We are currently banking on statistical predictive models to output information we <span class="textNudge">need</span> to be <a href="https://en.wikipedia.org/wiki/Determinism" target="_blank" alt="Determinism">deterministic</a>.\n<br>Currently, no ai is deterministic,\n<br>&nbsp;&nbsp; Sometimes ... Often yielding \'<a href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)" target="_blank" alt="AI Hallucinations">hallucinations</a>\', as the term for it.\n<br>\n<br>To me, there is a mix of Dreams and Hallucinations in AI.\n<br>\'Hallucinations\' are misinturpreted input to the brain,\n<br>\'Dreams\' are hallucinations the brain made itself.\n<br>&nbsp;&nbsp; Well, for my take on it.\n<br>\n<br>Cause Dreams are the random firings that our brain connects to fabricate a story from the happenstance of signals.\n<br>As the Cortex rolls back in functionality, decision-making drops, causing odd actions/choices in dreams.\n<br>&nbsp;&nbsp; The Limbic System goes into overdrive;\n<br>&nbsp;&nbsp;&nbsp;&nbsp; The Hippocampus & Amygdala kickin it up a notch.\n<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="textSkew">Our Memory & Stress center, respectively.</span>\n<br>\n<br>Dreams are a place we simulate outcomes to scenarios,\n<br>&nbsp;&nbsp; Emotional memories are smoothed out,\n<br>&nbsp;&nbsp;&nbsp;&nbsp; Concepts are conceived and broken down.\n<br>I love dreaming, but I dream in black\'n\'white or grayscale,\n<br>&nbsp;&nbsp; That\'s a story for another blog entry.\n<br>&nbsp;&nbsp;&nbsp;&nbsp; Qualia is a fun topic.\n<br>\n<br>Weirdly, I\'m trying to think of an example of a network that would generate dreams.\n<br>&nbsp;&nbsp; But all I can think of is a GAN, and that\'s not entirely right.\n<br>I\'d say, any AI that doesn\'t requires a "lighthouse" to guide it\'s learning/generation.\n<br>\n<br>I call it the "Lighthouse" Noise to myself,\n<br>&nbsp;&nbsp; I\'m sure there is an official term for it that I never heard of.\n<br>But learning and generation requires noise of some kind to instigate a pattern/prediction.\n<br>&nbsp;&nbsp; So I call it a "lighthouse" since it\'s the reference noise it had to start with initially.\n<br>In many learning processes, it\'s a random noise pattern of some kind.\n<br>&nbsp;&nbsp; But that noise pattern must remain the same throughout the learning process.\n<br>&nbsp;&nbsp;&nbsp;&nbsp; <span class="textSkew">That\'s fun to mess with mid-training, by the way.</span>\n<br>\n<br>I wasn\'t a fan of that aspect of AI learning in my ESN, LSTM, and GRU networks I\'ve built.\n<br>&nbsp;&nbsp; Feels like a needless bias that would limit possible predictions.\n<br>But that\'s just how those networks work.\n<br>&nbsp;&nbsp; Predictions are made based on the distance from their "lighthouse".\n<br>\n<br>If someone knows the correct term for it, I\'d love to know!\n<br>&nbsp;&nbsp; But I\'m not going to be adding comments to my blog,\n<br>&nbsp;&nbsp;&nbsp;&nbsp; So I dunno\n<br>\n<div class="blogSpacer"></div>\n\nOur brains work at "<a href=\'https://en.wikipedia.org/wiki/Critical_brain_hypothesis\' target="_blank" alt=\'Critical Brain Hypothesis\'>criticality</a>" or critical mass when awake and in REM sleep,\n<br>If our brain\'s operate beyond critical mass, we have a seizure.\n<br>If it falls below, we have a stroke or cognative issues.\n<br>Our brains,\n<br>&nbsp;&nbsp; Running on the "<a href=\'https://en.wikipedia.org/wiki/Edge_of_chaos\' target="_blank" alt=\'Edge of Chaos\'>edge of chaos</a>".\n<br>\n<br>AI is currently a similar beast,\n<br>Our current fleet of AIs work at critical mass for compute,\n<br>&nbsp;&nbsp; With the target of critical mass for data flow in "thought"\n<br>with added limiters/gates within LLMs to keep them from breaking themselves.\n<br>\n<br>Thing is, we are limited by training all ai structures too far <span class="textSkew">or</span> not far enough,\n<br>&nbsp;&nbsp; Ending with an unusable model.\n<br>\n<br>But, to state, I\'m a hobbyist ai researcher now.\n<br>&nbsp;&nbsp; Focusing on GAT networks.\n<br>&nbsp;&nbsp;&nbsp;&nbsp; And mention my Boid/Crowd ai work experiance on my About Me page.\n<br>\n<div class="blogSpacer"></div>\n\nThe current holy grail is Long-term Attention to Forethought;\n<br>&nbsp;&nbsp; Reason & Logical breakdown of a(n) thought/action/idea, inferencing.\n<br>&nbsp;&nbsp;&nbsp;&nbsp; An ai <a href=\'https://en.wikipedia.org/wiki/Prefrontal_cortex\' target="_blank" alt=\'Prefrontal Cortex\'>Prefrontal Cortex</a>, if you will.\n<br>Something called a "<a href=\'https://en.wikipedia.org/wiki/Spiking_neural_network\' target="_blank" alt=\'Spiked Neural Network\'>Spiked Neural Network</a>" is what we\'d label our own Human Brain as.\n<br>\n<br>The "Spike" is basically a handshake between neuronal firings.\n<br>&nbsp;&nbsp; A little "hey there" back to the neuron that initiated the synaptic firing.\n<br>&nbsp;&nbsp;&nbsp;&nbsp; Nitrogen playing heavily in Neuotransmitters.\n<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="textSkew">If you thought of whip-its, keep that in mind for the next section</span>\n<br>\n<br>In current day AI, the most similar behaviour is <a href=\'https://en.wikipedia.org/wiki/Backpropagation\'>Backpropagation</a>, but it\'s not the same.\n<br>&nbsp;&nbsp; But it\'s close enough we call it an analogue to spiking behaviour.\n<br>\n<br>Spiking behaviour is what\'s next for AI development,\n<br>&nbsp;&nbsp; More specifically, the behaviour that happens due to the spiking.\n<br>\n<br>Yet, I believe, we already have the ai neural structure to make spiking-adjacent behaviour to occur.\n<br>\n<br>Graph Neural Networks\n<br>&nbsp;&nbsp; Sadly, mostly used as "Recommendation Systems" or linking <span class="textSkew">associations</span>.\n<br>&nbsp;&nbsp;&nbsp;&nbsp; ::nudge nudge::\n<br>\n<br>It has the potential to free us from needing an <a href="https://www.geeksforgeeks.org/adam-optimizer/" target="_blank" alt="Adam Optimization">Adam Optimization</a> or a <a href="https://www.ibm.com/topics/gradient-descent" target="_blank" alt="Gradient Descent">Gradient Descent</a>.\n<br>&nbsp;&nbsp; But this is where my current research is,\n<br>&nbsp;&nbsp;&nbsp;&nbsp; So take me with a grain of salt.\n<br>\n<div class="blogSpacer"></div>\n\nIn our brain\'s Cortex, a set <span class="textNudge">Structure</span> of neurons along with their <span class="textNudge">Activity</span> is used to determine an outcome.\n<br>&nbsp;&nbsp; Of course this thought has lead me to believe Deja Vu is due to found "<a href=\'https://medium.com/@kdorichev/edges-detection-in-computer-vision-using-convolutions-5332efad3c91\' target=\'_blank\' alt=\'Edge Detection\'>convolved</a>" edges within the Cortex.\n<br>&nbsp;&nbsp;&nbsp;&nbsp; Pulling the "edge" of a dream you had that used the same neural pathways one time weeks back.\n<br>\n<br>Calling it \'convoled\' is a bit of a stretch,\n<br>&nbsp;&nbsp; But it makes sense to me.\n<br>Sartre lumped it in with "Quasi-observation",\n<br>&nbsp;&nbsp; Knowing everything about something all at once.\n<br>&nbsp;&nbsp;&nbsp;&nbsp; But we call it \'Deja Vu\'\n<br>\n<br>But within the brain, are known pathways,\n<br>&nbsp;&nbsp; Deterministic pathways between non-deterministic regions.\n<br>&nbsp;&nbsp;&nbsp;&nbsp; Time guiding the "simulation" of our mental frequencies of thoughts to a conclusion.\n<br>\n<div class="blogSpacer"></div>\n\nDynamic Graph Attention Networks ( dynamic GATs ) are the only thing that makes sense for true meta-learning\n<br>&nbsp;&nbsp; To me...\n<br>\n<br>Ya know, given what limited "classifications" I heard for "General Artificial Intelligence" recently....\n<br>Fine, I\'ll call what I want to do "super intelligence" then, or what ever.\n<br>&nbsp;&nbsp; Stupid labels...\n<br>\n<br>We haven\'t even agree\'d on what general artificial intelligence is yet,\n<br>&nbsp;&nbsp; But keep hearing weak sauce descriptions of it.\n<br>Hearing it described like "GPT+" or "LLM+" .... and not something with intrinsic meta-learning build in?\n<br>&nbsp;&nbsp; I don\'t care how many hidden layers you have,\n<br>&nbsp;&nbsp;&nbsp;&nbsp; I doubt that would ever reach general intelligence.\n<br>\n<br>If an AI can\'t generate a new thought or idea from a blank set of data,\n<br>&nbsp;&nbsp; The lack of a "lighthouse"\n<br>I\'m probably not going to call it general intelligence.\n<br>\n<br>If the ai can\'t Dream, it\'s not general intelligence.\n<br>\n<div class="blogSpacer"></div>\n\nThe progress towards useful AI has been slow.\n<br>&nbsp;&nbsp; For what AI can do for us today.\n<br>Yet, the progress towards AI integration has been quick and dirty.\n<br>\n<br>We need the "<a href=\'https://www.allrecipes.com/article/history-of-boxed-cake-mix/\' target=\'_blank\' alt=\'Duff Baking Boxs\'>Add the egg back to the cake recipe</a>" moment.\n<br>\n<br>I\'ve using AI as individual tools for my projects,\n<br>&nbsp;&nbsp; Running individual models for specific tasks as needed.\n<br>But seems most products are attempting all-in-one solutions.\n<br>\n<br>Best thing I\'ve seen so far is a camera app for your phone\n<br>&nbsp;&nbsp; That organizes your photos for you based on content in the photo\n<br>&nbsp;&nbsp;&nbsp;&nbsp; and creates keywords from a description of the photo.\n<br>&nbsp;&nbsp; An additive experience in life, not a replacement of a hobby or skill.\n<br>\n<br>\n<br>Yet the rush to impress the stockholders is going to be the death of us.\n<br>&nbsp;&nbsp; People interested in a company for monetary gain,\n<br>&nbsp;&nbsp;&nbsp;&nbsp; Is not going to have the best interest of the developers and consumers in mind.\n<br>\n<div class="blogSpacer"></div>\n\nThe socialites see AI as the solution to everything,\n<br>&nbsp;&nbsp; "Just use some AI to solve that problem."\n<br>\n<br>Until whichever tomorrow it is,\n<br>We\'ll be stuck with "adding glue to pizza" and "pregnant women should smoke 1 cigarette a day."\n<br>&nbsp;&nbsp; In it\'s own way, every product that releases will have something.\n<br>&nbsp;&nbsp;&nbsp;&nbsp; Why we get Video Game day-1 patches.\n<br>\n<br>A future time when the bottom dollar doesn\'t matter more than the consumers.\n<br>\n<br>In the mean time, we\'ll be punished for being online,\n<br>Assaulted by the woes of the ai generative community.\n<br>&nbsp;&nbsp; Bots on auto-pilot, reacting to themselves on young platforms,\n<br>&nbsp;&nbsp;&nbsp;&nbsp; Because by the time a platform gets big, it\'ll be too late to moderate.\n<br>\n<br>Hacking wont stop,\n<br>&nbsp;&nbsp; I\'m assuming people will have fun breaking anti-bot systems.\n<br>&nbsp;&nbsp;&nbsp;&nbsp; An afternoon project for the heck of it!\n<br>Except now, the hacking isn\'t illigal,\n<br>&nbsp;&nbsp; Its just social hacking using technology to operate as a person.\n<br>Its the same thinking as Hacking,\n<br>&nbsp;&nbsp; But in a different medium.\n<br>"How do I break this system?"\n<br>&nbsp;&nbsp; Remains the same question.\n<br>If that doesn\'t paint me as gray-hat minded,\n<br>&nbsp;&nbsp; I dunno what will.\n<br>\n<br>The ai doors have been opened,\n<br>&nbsp;&nbsp; No puttin\' the blood back in the elevator now.\n<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \'All work and no play makes Jack a dull boy\'\n<br>\n<div class="blogSpacer"></div>\n\nWe are embracing the monetary gain from that hope of AI.\n<br>Which I\'m all for, but I think the path we are taking needs to be expanded further before we can continue safely.\n<br>\n<br>It feels like the guys that invented the wheels on ai,\n<br>&nbsp;&nbsp; Watched as their project roll out the door,\n<br>&nbsp;&nbsp;&nbsp;&nbsp; Without them at the wheel.\n<br>\n<br>Google\'s <a href=\'https://gamengen.github.io/\' target=\'_blank\' alt=\'GameNGen\'>GameNGen</a> and <a href=\'https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/\' target=\'_blank\' alt=\'Genie 2\'>Genie 2</a> now\n<br>&nbsp;&nbsp; Feels more like a "We can do this, so why not?" type of projects\n<br>But Google is known for exploring the unknown for fun.\n<br>\n<br>&nbsp; -Kevin Edzenga\n';const blogEntryObj=new blogEntry(null,entryTitle,entryDate,entryTags,entryBody);export{blogEntryObj};