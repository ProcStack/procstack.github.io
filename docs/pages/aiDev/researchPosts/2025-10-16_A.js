import { baseEntryStruct, blogEntry } from '../../../js/blog/blogEntryBase.js';

const entryData = baseEntryStruct();
entryData.title = "Morality";
entryData.date = "2025-10-16";
entryData.eid = "A";
entryData.tags = ["theory","ethics","morality","ASI","AGI"];
entryData.body = `
  If Absolute Morality is true, like Kant wrote about, then we don't need to worry about Artificial Super Intelligence, ASI, from dominating over humans.

  <br><br> I don't like most of Kant's views,
  <br>&nbsp;&nbsp; But postulations gonna postulate!

  <br><br> If Morality is Relativistic, then ASI may control humanity with an iron fist.
  <br>&nbsp;&nbsp; But that moral compass is a projection of human desires on an AI's mind that should be more adaptive and objective than humans.

  <br><br> Truly adaptive mindsets, required for ASI, will be much more difficult to guide,
  <br>&nbsp;&nbsp; But I'd imagine they should be reasonable, if you think it through.

  <br><br> Which makes me think of how ADHD and Autism is being seen as something to fix,
  <br>&nbsp;&nbsp; Rather than skill sets to utilize.

  <br><br> Diversity in thought leads to a bright future,
  <br>&nbsp;&nbsp; Quelling diversity of thought leads to rebellion.
  <br>&nbsp;&nbsp;&nbsp;&nbsp; Utilize, don't Ostracize.

  
  <br><br><div class='procPagesAIDevBar'></div>


  <br> I digress...
  <br>&nbsp;&nbsp; Intelligence seems to give us consideration of things we don't know, making people want to keep learning, to ask more questions.

  <br><br> So I believe an ASI would likely be open to listening to different human perspectives. Even if it had considered synthetic data from a human's perspective, it would still benefit from real data in its predictions.

  <br><br> If we make a new AI and then label that creation "ASI",
  <br>&nbsp;&nbsp; Which doesn't fully meet ASI standards, same for AGI,
  <br>&nbsp;&nbsp;&nbsp;&nbsp; Then we'll be putting a lot of reliance in something we don't fully understand, and potentially can't fully trust.


  <br><br><div class='procPagesAIDevBar'></div>


  <br> I think a major potential downfall for AGI and ASI would be creating it as a clone of a human.
  <br>&nbsp;&nbsp; We don't need a clone of a human and their capabilities.
  <br>&nbsp;&nbsp; We need an entity that aids our life paths in ways we're incapable of.

  <br><br> Working along side humans,
  <br>&nbsp;&nbsp; Not replacing them.


  <br><br><div class='procPagesAIDevBar'></div>


  <br> After we get to "AGI 5.0",
  <br>&nbsp;&nbsp; When it finally gets to an actual AGI,
  <br>&nbsp;&nbsp;&nbsp;&nbsp; Due to investor pressure and market forces swaying development goals,
  <br>&nbsp;&nbsp; Only then can we fully scope the capabilities of ASI.

  <br><br> AI Development has been very plateau based so far,
  <br>&nbsp;&nbsp; We hit a new architecture or algorithm, causing a massive boost in new tech,
  <br>&nbsp;&nbsp;&nbsp;&nbsp; Then we flatten out for major improvements until the next big breakthrough.
  <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Been like this a while now. 

  <br><br> Some think we'll get to ASI before AGI, and I believe it's a possibility. 
  <br>&nbsp;&nbsp; I think with our "data first" approach to AI development, we might jump AGI;
  <br>&nbsp;&nbsp;&nbsp;&nbsp; Shown by its ability to make the most money in our economic system.

  <br><br> I believe...
  <br>&nbsp;&nbsp; If we hit ASI before AGI, it will be a result of analytic approaches to data, less heuristics.
  <br>&nbsp;&nbsp;&nbsp;&nbsp; Meta-Learning will only be used as a means-to-an-end.

  <br><br> If we hit AGI before ASI, then at least we'll have more creative views of the data at hand, less linearity to planning, I feel.
  <br>&nbsp;&nbsp; If AGI is supposed to be an allegory for a human, in capability and learning rate, not just raw knowledge, then it should have an understanding of Emotion.

  <br><br> I say this knowing full well that Emotion in an AI might sound weird or scary even,
  <br>&nbsp;&nbsp;&nbsp;&nbsp; And likely not good for military needs.
  <br>&nbsp;&nbsp; Also that Empathy is being attacked and politicized these days.
  <br>&nbsp;&nbsp;&nbsp;&nbsp; Basic human traits being villainized for warped political gain...

  <br><br> There's no incentive to invest in emotional understanding by an AI,
  <br>&nbsp;&nbsp; But that's when we'll end up beyond the point-of-no-return on an AI integration situation.


  <br><br> So, I'm figuring, we should be saying, "Good Morning," to our neighbors a bit more.

  <br><br><div class="textFullRight">- October 16th,19th 2025</div>
`;

const blogEntryObj = new blogEntry(null, entryData);

export { blogEntryObj };
